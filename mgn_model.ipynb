{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score, precision_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, MessagePassing\n",
    "from torch_geometric.nn import GATConv, GCNConv, GINConv, SAGEConv, RGCNConv\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, feed_forward_embedding, activation_function, dropout_p):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        num_classes = 2\n",
    "\n",
    "        self.input_layer = torch.nn.Linear(num_features, hidden)\n",
    "        self.sage_layers = torch.nn.ModuleList(\n",
    "            [SAGEConv(hidden, hidden) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(hidden, num_classes)\n",
    "\n",
    "        self.feed_forward_embedding = feed_forward_embedding\n",
    "        if feed_forward_embedding:\n",
    "            self.embedding = torch.nn.Embedding(53, hidden)\n",
    "\n",
    "        if activation_function == 'relu':\n",
    "          self.ac = torch.nn.ReLU()\n",
    "        elif activation_function == 'leaky_relu':\n",
    "          self.ac = torch.nn.LeakyReLU(negative_slope=0.01)\n",
    "        elif activation_function == 'sigmoid':\n",
    "          self.ac = torch.nn.Sigmoid()\n",
    "        elif activation_function == 'tanh':\n",
    "          self.ac = torch.nn.Tanh()\n",
    "        elif activation_function == 'elu':\n",
    "          self.ac = torch.nn.ELU(alpha=1.0)\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, data):\n",
    "        # x, edge_index, batch = data.x.to(torch.float32), data.edge_index, data.batch\n",
    "\n",
    "        # x = F.relu(self.input_layer(x))\n",
    "\n",
    "        if self.feed_forward_embedding:\n",
    "            x, edge_index, batch = data.x.to(torch.long), data.edge_index, data.batch\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            x, edge_index, batch = data.x.to(torch.float32), data.edge_index, data.batch\n",
    "            x = self.ac(self.input_layer(x))\n",
    "\n",
    "        for sage_layer in self.sage_layers:\n",
    "            x = self.ac(sage_layer(x, edge_index))\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Dataset\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "\n",
    "def get_dataset(graphs):\n",
    "    data_list = []\n",
    "    for graph in graphs.values():\n",
    "        data = from_networkx(graph)\n",
    "        data.y = torch.tensor([graph.graph['label']], dtype=torch.long)\n",
    "        # data.x = graph.node_features\n",
    "        data.x = torch.tensor(graph.node_features)\n",
    "        data_list.append(data)\n",
    "\n",
    "    dataset = CustomGraphDataset(data_list)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def cross_validation_with_val_set(dataset, model, folds, epochs, batch_size,\n",
    "                                  lr, lr_decay_factor, lr_decay_step_size,\n",
    "                                  weight_decay, logger=None, info=None, save_model=False):\n",
    "    model_name = info['name']\n",
    "    layer = info['layer']\n",
    "    hidden = info['hidden']\n",
    "    word = info['word_embedding']\n",
    "    ac = info['ac']\n",
    "    dp = info['dp']\n",
    "    print(model_name, layer, hidden,ac, dp)\n",
    "    test_losses, train_accs, test_accs, durations = [], [], [], []\n",
    "\n",
    "\n",
    "    # best_acc, best_pre, best_recall, best_f1, best_auc\n",
    "    train_best_result = [0, 0, 0, 0, 0]\n",
    "    test_best_result = [0, 0, 0, 0, 0]\n",
    "\n",
    "    train_results = {\"loss\": [], 'std': []}\n",
    "    test_results = {\"loss\": [], \"accuracy\": [], \"recall\": [], \"precision\": [], \"f1\": [], \"auc\": [], 'std': []}\n",
    "    # Define the directory where the results will be saved\n",
    "    results_dir = os.path.expanduser('/content/drive/MyDrive/malware_project/results')\n",
    "    filename = f\"{model_name}_dropout_{dp}_results.json\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    for fold, (train_idx, test_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "\n",
    "        train_dataset = dataset[train_idx]\n",
    "        test_dataset = dataset[test_idx]\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "        if fold == 0:\n",
    "          epochs = 200\n",
    "        else:\n",
    "          epochs = 100\n",
    "\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc, train_precision,train_recall, train_f1, train_auc, train_std = train(model, optimizer, train_loader)\n",
    "\n",
    "            test_loss, test_accuracy, test_recall, test_precision, test_f1, test_auc, test_std = eval_metrics(model, test_loader)\n",
    "\n",
    "            # best_acc, best_pre, best_recall, best_f1, best_auc\n",
    "            train_best_result = [0, 0, 0, 0, 0]\n",
    "            test_best_result = [0, 0, 0, 0, 0]\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            for best_metric in test_best_result:\n",
    "                if train_acc > train_best_result[0]:\n",
    "                    train_best_result[0] = train_acc\n",
    "\n",
    "                if test_accuracy > test_best_result[0]:\n",
    "                    test_best_result[0] = test_accuracy\n",
    "\n",
    "                if train_precision > train_best_result[1]:\n",
    "                    train_best_result[1] = train_precision\n",
    "\n",
    "                if test_precision > test_best_result[1]:\n",
    "                    test_best_result[1] = test_precision\n",
    "\n",
    "                if train_recall > train_best_result[2]:\n",
    "                    train_best_result[2] = train_recall\n",
    "\n",
    "                if test_recall > test_best_result[2]:\n",
    "                    test_best_result[2] = test_recall\n",
    "\n",
    "                if train_f1 > train_best_result[3]:\n",
    "                    train_best_result[3] = train_f1\n",
    "\n",
    "                if test_f1 > test_best_result[3]:\n",
    "                    test_best_result[3] = test_f1\n",
    "\n",
    "                if train_auc > train_best_result[4]:\n",
    "                    train_best_result[4] = train_auc\n",
    "\n",
    "                if test_auc > test_best_result[4]:\n",
    "                    test_best_result[4] = test_auc\n",
    "\n",
    "            # Record metrics\n",
    "            train_results[\"loss\"].append(train_loss)\n",
    "            train_results[\"std\"].append(train_std)\n",
    "\n",
    "            test_results[\"loss\"].append(test_loss)\n",
    "            test_results[\"accuracy\"].append(test_accuracy)\n",
    "            test_results[\"recall\"].append(test_recall)\n",
    "            test_results[\"precision\"].append(test_precision)\n",
    "            test_results[\"f1\"].append(test_f1)\n",
    "            test_results[\"auc\"].append(test_auc)\n",
    "            test_results[\"std\"].append(test_std)\n",
    "            with open(os.path.join(results_dir, f\"train_{filename}\"), 'w') as f:\n",
    "              json.dump(train_results, f)\n",
    "            with open(os.path.join(results_dir, f\"test_{filename}\"), 'w') as f:\n",
    "              json.dump(test_results, f)\n",
    "\n",
    "            eval_info = {\n",
    "                'fold': fold,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': test_losses[-1],\n",
    "                'train_acc': train_acc,\n",
    "                'train_recall': train_recall,\n",
    "                'train_f1':train_f1,\n",
    "                'train_auc': train_auc,\n",
    "                'test_acc': test_accuracy,\n",
    "                'test_recall': test_recall,\n",
    "                'test_f1': test_f1,\n",
    "                'test_auc': test_auc,\n",
    "                'train_std': train_std,\n",
    "                'test_std' : test_std\n",
    "            }\n",
    "\n",
    "            if logger is not None:\n",
    "                logger(eval_info)\n",
    "\n",
    "            if epoch % lr_decay_step_size == 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    new_lr =  lr_decay_factor * param_group['lr']\n",
    "                    print(new_lr)\n",
    "                    param_group['lr'] = new_lr\n",
    "            end_time = time.time()\n",
    "            epoch_duration = end_time - start_time\n",
    "            eval_info['time'] = epoch_duration\n",
    "            print(eval_info)\n",
    "\n",
    "        print(\"Best train results: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}, AUC: {:.4f}\".format(\n",
    "            train_best_result[0], train_best_result[1], train_best_result[2], train_best_result[3], train_best_result[4]))\n",
    "\n",
    "        print(\"Best test results: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}, AUC: {:.4f}\".format(\n",
    "            test_best_result[0], test_best_result[1], test_best_result[2], test_best_result[3], test_best_result[4]))\n",
    "\n",
    "        if save_model:\n",
    "          model_save_path = f\"/content/drive/MyDrive/malware_project/model/model_{model_name}_fold_{fold+1}.pt\"\n",
    "          torch.save(model.state_dict(), model_save_path)\n",
    "          print(\"Model saved at\", model_save_path)\n",
    "\n",
    "    return train_best_result, test_best_result\n",
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for train_idx, test_idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        train_indices.append(torch.from_numpy(train_idx))\n",
    "        test_indices.append(torch.from_numpy(test_idx))\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    predicted_scores = []\n",
    "\n",
    "    total_loss = 0\n",
    "    total_square_loss = 0\n",
    "    for data in loader:\n",
    "      optimizer.zero_grad()\n",
    "      data = data.to(device)\n",
    "      out = model(data)\n",
    "      loss = F.nll_loss(out, data.y.view(-1))\n",
    "      loss.backward()\n",
    "      total_loss += loss.item() * num_graphs(data)\n",
    "      total_square_loss += (loss.item() * num_graphs(data)) ** 2\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      out = out.detach().cpu()\n",
    "      pred = out.max(1)[1]\n",
    "      pred_prob = torch.sigmoid(out)[:, 1].cpu().numpy()\n",
    "\n",
    "\n",
    "      correct += pred.eq(data.y.cpu().view(-1)).sum().item()\n",
    "      true_labels.extend(data.y.cpu().numpy())\n",
    "      predicted_labels.extend(pred.cpu().numpy())\n",
    "      predicted_scores.extend(pred_prob)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    loss_variance = total_square_loss/len(loader) - (avg_loss)**2\n",
    "    loss_std = math.sqrt(loss_variance)\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    auc_roc = roc_auc_score(true_labels, predicted_scores)\n",
    "\n",
    "    return avg_loss, accuracy, precision,recall, f1, auc_roc, loss_std\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        loss += F.nll_loss(out, data.y.view(-1), reduction='sum').item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "def eval_metrics(model, loader):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    predicted_scores = []\n",
    "\n",
    "    total_loss = 0\n",
    "    total_square_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            pred = output.max(1)[1]\n",
    "            pred_prob = torch.sigmoid(output)[:, 1].cpu().numpy()\n",
    "            loss = F.nll_loss(output, data.y.view(-1))\n",
    "            total_loss += loss.item() * num_graphs(data)\n",
    "            total_square_loss += (loss.item() * num_graphs(data)) ** 2\n",
    "\n",
    "\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "        true_labels.extend(data.y.cpu().numpy())\n",
    "        predicted_labels.extend(pred.cpu().numpy())\n",
    "        predicted_scores.extend(pred_prob)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    loss_variance = total_square_loss/len(loader) - (avg_loss)**2\n",
    "    loss_std = math.sqrt(loss_variance)\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    auc_roc = roc_auc_score(true_labels, predicted_scores)\n",
    "\n",
    "    return avg_loss, accuracy, precision,recall, f1, auc_roc, loss_std\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, data_list, transform=None, pre_transform=None):\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        class DataY:\n",
    "            def __init__(self, data_list):\n",
    "                self.data_list = data_list\n",
    "\n",
    "            @property\n",
    "            def y(self):\n",
    "                return torch.tensor([data.y.item() for data in self.data_list], dtype=torch.long)\n",
    "\n",
    "        return DataY(self.data_list)\n",
    "\n",
    "def save_experiment_results(directory, word_embedding, layers, hidden, model, train_metrics, test_metrics):\n",
    "     # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Set the CSV file path\n",
    "    csv_file_path = os.path.join(directory, 'experiment_results.csv')\n",
    "    # Extract metrics\n",
    "    train_accuracy, train_precision, train_recall, train_f1_score, train_auc_roc = train_metrics\n",
    "    test_accuracy, test_precision, test_recall, test_f1_score, test_auc_roc = test_metrics\n",
    "\n",
    "    # Prepare the row data\n",
    "    row_data = [word_embedding, layers, hidden, model, train_accuracy, train_precision, train_recall, train_f1_score, train_auc_roc,\n",
    "                    test_accuracy, test_precision, test_recall, test_f1_score, test_auc_roc]\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Write the row data to the CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if not file_exists:\n",
    "            # Write the header if the file doesn't exist\n",
    "            header = ['Word Embedding', 'Layers', 'Hidden', 'Model', 'Train_Accuracy', 'Train_Precision', 'Train_Recall', 'Train_F1-score', 'Train_AUC-ROC', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1-score', 'Test_AUC-ROC']\n",
    "            writer.writerow(header)\n",
    "\n",
    "        # Write the row data\n",
    "        writer.writerow(row_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import json\n",
    "import os\n",
    "\n",
    "def logger(info):\n",
    "    fold, epoch = info['fold'] + 1, info['epoch']\n",
    "    train_loss, val_loss = info['train_loss' ], info['val_loss']\n",
    "    train_std, test_std = info['train_std'], info['test_std']\n",
    "    train_acc, train_recall, train_f1, train_auc = info['train_acc'], info['train_recall'], info['train_f1'], info['train_auc']\n",
    "    test_acc, test_recall, test_f1, test_auc = info['test_acc'], info['test_recall'], info['test_f1'], info['test_auc']\n",
    "    print('{:02d}/{:03d}: Train_loss: {:.4f}, Val Loss: {:.4f}, Train Accuracy: {:.3f}, Test Accuracy: {:.3f},\\\n",
    "    Train Recall: {:.3f}, Test Recall: {:.3f}, Train F1-Score: {:.3f}, Test F1-Score: {:.3f},\\\n",
    "    Train Auc_Roc: {:.3f}, Test Auc_Roc: {:.3f}, train_std: {:.3f}, test_std: {:.3f}'.format(fold, epoch, train_loss, val_loss, train_acc, test_acc,\n",
    "                                                        train_recall, test_recall, train_f1, test_f1, train_auc, test_auc, train_std, test_std)\n",
    "    )\n",
    "\n",
    "epochs = 200\n",
    "layers = [6]\n",
    "hiddens = [64]\n",
    "lr_decay_factor = 0.2\n",
    "lr_decay_step_size = 50\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "folds = 5\n",
    "\n",
    "\n",
    "# nets = ['GIN', 'SAGE', 'GCN']\n",
    "nets = ['SAGE']\n",
    "# activation_functions = ['relu', 'leaky_relu', 'sigmoid', 'tanh', 'elu']\n",
    "activation_functions = ['leaky_relu']\n",
    "dropout_p = [0.2]\n",
    "# word_embeddings = ['one_hot']\n",
    "csv_file_path = '/content/drive/MyDrive/malware_project/'\n",
    "\n",
    "def release_model(model):\n",
    "    # Move the model to CPU\n",
    "    model.to('cpu')\n",
    "\n",
    "    # Delete the model and its variables\n",
    "    del model\n",
    "\n",
    "    # Clear the GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "word_embedding = 'one_hot'\n",
    "dataset = datasets_one_hot\n",
    "layer = 12\n",
    "hidden = 128\n",
    "for net, ac, dp in product(nets, activation_functions, dropout_p):\n",
    "  if net == 'GCN':\n",
    "    model = GCNModel(datasets_one_hot, layer, hidden, False, ac, dp)\n",
    "  elif net == 'GCN':\n",
    "    model = GINModel(datasets_one_hot, layer, hidden, False, ac, dp)\n",
    "  else:\n",
    "    model = GraphSAGEModel(datasets_one_hot, layer, hidden, False, ac, dp)\n",
    "  info = {'name':net, 'layer': layer, 'hidden': hidden, 'word_embedding': word_embedding, 'ac': ac, 'dp': dp}\n",
    "  print(info)\n",
    "  train_best_result, test_best_result = cross_validation_with_val_set(\n",
    "      dataset,\n",
    "      model,\n",
    "      folds=folds,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      lr=lr,\n",
    "      lr_decay_factor=lr_decay_factor,\n",
    "      lr_decay_step_size=lr_decay_step_size,\n",
    "      weight_decay=0,\n",
    "      logger= logger,\n",
    "      info=info\n",
    "  )\n",
    "  release_model(model)\n",
    "\n",
    "  save_experiment_results(csv_file_path, word_embedding, layer, hidden, net, train_best_result, test_best_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}